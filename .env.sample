# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2
OLLAMA_CONTEXT_LENGTH=4096

# Ollama API Key (for https://ollama.com cloud API)
OLLAMA_API_KEY=

# Provider Selection (ollama|llamacpp|vllm|other|claude|gemini|codex|hf)
QARIN_CLI_PROVIDER=ollama

# Cloud Ollama (optional, legacy)
OLLAMA_CLOUD_HOST=
OLLAMA_CLOUD_API_KEY=

# Alternative Provider API Keys
ANTHROPIC_API_KEY=
GEMINI_API_KEY=
OPENAI_API_KEY=
HF_TOKEN=
SEARCH_API_KEY=
SEARCH_API_PROVIDER=tavily

# GitHub Token (for hooks and integrations)
GH_TOKEN=

# llama.cpp Server (cross-platform: CPU/CUDA/Metal/Vulkan)
LLAMACPP_HOST=http://localhost:8080
LLAMACPP_API_KEY=
LLAMACPP_MODEL=default

# vLLM Server (tensor parallelism for multi-GPU inference)
VLLM_HOST=http://localhost:8000
VLLM_API_KEY=
VLLM_MODEL=default
VLLM_TENSOR_PARALLEL_SIZE=1

# Other Provider (any OpenAI-compatible endpoint -- set URL, API key optional)
OTHER_PROVIDER_HOST=
OTHER_PROVIDER_API_KEY=
OTHER_PROVIDER_MODEL=default

# Provider Routing (per task type)
QARIN_CLI_CODING_PROVIDER=ollama
QARIN_CLI_CODING_MODEL=codestral:latest
QARIN_CLI_AGENT_PROVIDER=ollama
QARIN_CLI_AGENT_MODEL=llama3.2
QARIN_CLI_SUBAGENT_PROVIDER=ollama
QARIN_CLI_SUBAGENT_MODEL=glm-ocr

# Agent-Specific Model Assignments (up to 10 agent types, mixed providers)
# Format: QARIN_CLI_AGENT_<TYPE>_PROVIDER and QARIN_CLI_AGENT_<TYPE>_MODEL
QARIN_CLI_AGENT_CODE_PROVIDER=ollama
QARIN_CLI_AGENT_CODE_MODEL=codestral:latest
QARIN_CLI_AGENT_REVIEW_PROVIDER=ollama
QARIN_CLI_AGENT_REVIEW_MODEL=llama3.2
QARIN_CLI_AGENT_TEST_PROVIDER=ollama
QARIN_CLI_AGENT_TEST_MODEL=llama3.2
QARIN_CLI_AGENT_PLANNING_PROVIDER=ollama
QARIN_CLI_AGENT_PLANNING_MODEL=llama3.2
QARIN_CLI_AGENT_DOCS_PROVIDER=ollama
QARIN_CLI_AGENT_DOCS_MODEL=llama3.2

# Feature Flags
AUTO_COMPACT=true
COMPACT_THRESHOLD=0.85
HOOKS_ENABLED=true
